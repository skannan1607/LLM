# -*- coding: utf-8 -*-
"""QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11A5iI0CcZ3NlDqHXb7sZ1LEytc0cfkOv
"""

from transformers import pipeline
qa_pipeline = pipeline("question-answering",model="deepset/roberta-base-squad2")
context = "Python was created by Guiddo von rossum in 1991 and the Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset"
question = ["When was created??","by whom","What is SQuAD?"]
for q in question:
  result = qa_pipeline(context=context,question=q)
  print(f"Question: {q}")
  print(f"Answer: {result['answer']} (score: {result['score']:.4f})")

import os
!pip install langchain-community
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(openai_api_base = "https://openrouter.ai/api/v1", openai_api_key = "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", model = "openai/gpt-3.5-turbo")
question = ["When was created??","by whom","What is SQuAD?"]
for q in question:
  res = llm.invoke(q)
  print(f"Question: {q}")
  print(f"Answer: {res.content}")

