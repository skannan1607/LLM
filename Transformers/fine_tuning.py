# -*- coding: utf-8 -*-
"""Fine tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_AtTjf27xzjimfWKJdfIl4PeY8vZA0xV
"""

!pip install transformers datasets evaluate sentencepiece

#T5 SMALL HAS 60 MILLION PARAMETERS AND T5 BASE HAS 220 MILLION PARAMETERS

from datasets import load_dataset

# From hugging face import samsum dataset has 14k samples of convos
dataset = load_dataset("knkarthick/samsum")
print("Dataset loaded:")
print(dataset)


print("\n--- Sample ---")
print("DIALOGUE:")
print(dataset["train"][0]["dialogue"])
print("\nSUMMARY:")
print(dataset["train"][0]["summary"])

from transformers import AutoTokenizer

# Load the T5-small tokenizer
tokenizer = AutoTokenizer.from_pretrained("t5-small")
prefix = "summarize: "

# T5's max length is 512
max_input_length = 512
max_target_length = 128

def preprocess_function_samsum(examples):
    # Prepare the inputs (the 'dialogue')
    inputs = [prefix + doc for doc in examples["dialogue"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding="max_length")

    # Prepare the "labels" (the 'summary')
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=max_target_length, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply the preprocessing
# Runs on all 14,732 train samples
tokenized_dataset = dataset.map(preprocess_function_samsum, batched=True)

print("\nTokenizing complete.")
print(tokenized_dataset)

from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

# Load the T5-small model
model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

# Define the training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./t5-samsum-results",    # Directory to save the model
    eval_strategy="epoch",         # Evaluate at the end of each epoch
    learning_rate=3e-4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    save_total_limit=3,
    report_to="none",                    # or uses wandib to show history
    num_train_epochs=8,                  # We'll train for 3 epochs
    predict_with_generate=True,          # Required for summarization metrics
    fp16=True,                           # Use 16-bit precision
)

# The data collator dynamically pads sequences to the max length in a batch
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

# Create the Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Start the training!
print("Starting T5 training on SAMSum...")
trainer.train()
print("Training finished.")

# Save the final model & tokenizer
trainer.save_model("./t5-samsum-model")

from transformers import pipeline

# Load the summarization pipeline with our fine-tuned model
summarizer_pipe = pipeline("summarization", model="./t5-samsum-model", tokenizer=tokenizer)

# Grab a dialogue from the test set
test_dialogue = dataset["test"][10]["dialogue"]

print("\n--- Testing with a new dialogue ---")
print("\nORIGINAL DIALOGUE:")
print(test_dialogue)

# Generate summary
summary = summarizer_pipe(test_dialogue, max_length=100, min_length=10, do_sample=False)

print("\nGENERATED SUMMARY:")
print(summary[0]['summary_text'])

print("\nACTUAL HUMAN SUMMARY:")
print(dataset["test"][10]["summary"])

# TEST IT WITH T5 small without finetunig and ask the same question

from transformers import pipeline, AutoTokenizer

# Load the ORIGINAL t5-small, NOT your finetuned one
tokenizer = AutoTokenizer.from_pretrained("t5-small")
summarizer = pipeline("summarization", model="t5-small", tokenizer=tokenizer)

# Get a dialogue from the test set
test_dialogue = """
Wanda: Let's make a party!
Gina: Why?
Wanda: beacuse. I want some fun!
Gina: ok, what do u need?
Wanda: 1st I need too make a list
Gina: noted and then?
Wanda: well, could u take yours father car and go do groceries with me?
Gina: don't know if he'll agree
Wanda: I know, but u can ask :)
Gina: I'll try but theres no promisess
Wanda: I know, u r the best!
Gina: When u wanna go
Wanda: Friday?
Gina: ok, I'll ask
"""

# Ask the original model to summarize it
print(summarizer(test_dialogue))